{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "17a00790-6811-467a-8738-c24e8c1f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 06:40:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean as _mean, lit\n",
    "from pyspark.sql.functions import col, lit, row_number, rand, when, isnan, count\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType, LongType, StringType\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from scrapy import Selector\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Heart Disease Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data/heart_disease.csv\", header = True, inferSchema = True)\n",
    "\n",
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "78f9c71b-402f-4ae0-9971-bb8c8294732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the desired columns\n",
    "selected_columns = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "\n",
    "# Selecting only the desired columns\n",
    "df = df.select(*selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "25e8a7cb-7ead-47f1-ac3e-35c3c5796230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "|age|sex|painloc|painexer| cp|trestbps|smoke|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "| 63|  1|      1|       1|  1|   145.0| null|  1|   0|   0|  0|       0|   10.5|  150.0|    0|    2.3|    3|     0|\n",
      "| 67|  1|      1|       1|  4|   160.0| null|  0|   1|   0|  0|       0|    9.5|  108.0|    1|    1.5|    2|     1|\n",
      "| 67|  1|      1|       1|  4|   120.0| null|  0|   1|   0|  0|       0|    8.5|  129.0|    1|    2.6|    2|     1|\n",
      "| 37|  1|      1|       1|  3|   130.0| null|  0|   1|   0|  0|       0|   13.0|  187.0|    0|    3.5|    3|     0|\n",
      "| 41|  0|      1|       1|  2|   130.0| null|  0|   0|   0|  0|       0|    7.0|  172.0|    0|    1.4|    1|     0|\n",
      "| 56|  1|      1|       1|  2|   120.0| null|  0|   0|   0|  0|       0|   11.3|  178.0|    0|    0.8|    1|     0|\n",
      "| 62|  0|      1|       1|  4|   140.0| null|  0|   0|   0|  0|       0|    6.0|  160.0|    0|    3.6|    3|     1|\n",
      "| 57|  0|      1|       1|  4|   120.0| null|  0|   0|   0|  0|       0|    9.0|  163.0|    1|    0.6|    1|     0|\n",
      "| 63|  1|      1|       1|  4|   130.0| null|  0|   1|   1|  0|       0|    8.0|  147.0|    0|    1.4|    2|     1|\n",
      "| 53|  1|      1|       1|  4|   140.0| null|  1|   1|   0|  0|       1|    5.5|  155.0|    1|    3.1|    3|     1|\n",
      "| 57|  1|      1|       1|  4|   140.0| null|  0|   0|   0|  0|       0|    8.2|  148.0|    0|    0.4|    2|     0|\n",
      "| 56|  0|      1|       1|  2|   140.0| null|  0|   1|   1|  0|       0|    4.5|  153.0|    0|    1.3|    2|     0|\n",
      "| 56|  1|      1|       1|  3|   130.0| null|  1|   0|   0|  0|       0|   13.0|  142.0|    1|    0.6|    2|     1|\n",
      "| 44|  1|      1|       1|  2|   120.0| null|  0|   1|   0|  0|       0|    9.3|  173.0|    0|    0.0|    1|     0|\n",
      "| 52|  1|      1|       1|  3|   172.0| null|  1|   0|   0|  0|       0|   12.5|  162.0|    0|    0.5|    1|     0|\n",
      "| 57|  1|      1|       1|  3|   150.0| null|  0|   0|   1|  0|       0|   11.0|  174.0|    0|    1.6|    1|     0|\n",
      "| 48|  1|      1|       1|  2|   110.0| null|  0|   1|   0|  0|       0|    9.8|  168.0|    0|    1.0|    3|     1|\n",
      "| 54|  1|      1|       1|  4|   140.0| null|  0|   0|   0|  0|       1|    7.8|  160.0|    0|    1.2|    1|     0|\n",
      "| 48|  0|      1|       1|  3|   130.0| null|  0|   0|   0|  0|       0|   10.0|  139.0|    0|    0.2|    1|     0|\n",
      "| 49|  1|      1|       1|  2|   130.0| null|  0|   0|   0|  0|       0|   12.0|  171.0|    0|    0.6|    1|     0|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_attributes = [\n",
    "    'painloc',      # Chest pain location (1 = substernal, 0 = otherwise)\n",
    "    'painexer',     # Whether pain is provoked by exertion (1 = yes, 0 = no)\n",
    "    'fbs',          # Fasting blood sugar > 120 mg/dL (1 = true, 0 = false)\n",
    "    'prop',         # Beta blocker used during exercise ECG (1 = yes, 0 = no)\n",
    "    'nitr',         # Nitrates used during exercise ECG (1 = yes, 0 = no)\n",
    "    'pro',          # Calcium channel blocker used during exercise ECG (1 = yes, 0 = no)\n",
    "    'diuretic',     # Diuretic used during exercise ECG (1 = yes, 0 = no)\n",
    "    'exang'\n",
    "]\n",
    "\n",
    "for column in binary_attributes:\n",
    "    # Calculate the mode of the binary column\n",
    "    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "    \n",
    "    # Replace non-binary and NaN values with the mode\n",
    "    df = df.withColumn(column, when((col(column).isNull()) | (~col(column).isin(0, 1)), mode_value).otherwise(col(column)))\n",
    "\n",
    "# Replace missing values in 'thaldur' column with the average of the column\n",
    "thaldur_average = df.agg(_mean(col('thaldur')).alias('mean')).first()['mean']\n",
    "df = df.withColumn('thaldur', when(col('thaldur').isNull(), thaldur_average).otherwise(col('thaldur')))\n",
    "\n",
    "# Replace missing values in 'thalach' column with the average of the column\n",
    "thalach_average = df.agg(_mean(col('thalach')).alias('mean')).first()['mean']\n",
    "df = df.withColumn('thalach', when(col('thalach').isNull(), thalach_average).otherwise(col('thalach')))\n",
    "\n",
    "# Replace missing values in 'trestbps' column with the average of the column\n",
    "trestbps_average = df.agg(_mean(col('trestbps')).alias('mean')).first()['mean']\n",
    "df = df.withColumn('trestbps', when(col('trestbps').isNull(), trestbps_average).otherwise(col('trestbps')))\n",
    "\n",
    "# Calculate the average of the 'oldpeak' column\n",
    "average_oldpeak = df.agg(_mean(col('oldpeak')).alias('mean')).first()['mean']\n",
    "\n",
    "# Replace missing values, values less than 0, and values greater than 4 with the average\n",
    "df = df.withColumn('oldpeak', when(col('oldpeak').isNull() | (col('oldpeak') < 0) | (col('oldpeak') > 4), average_oldpeak).otherwise(col('oldpeak')))\n",
    "\n",
    "valid_categories = {\n",
    "    'cp': {1, 2, 3, 4},\n",
    "    'slope': {1, 2, 3},\n",
    "}\n",
    "\n",
    "for column, valid_set in valid_categories.items():\n",
    "    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "    df = df.withColumn(column, when(~col(column).isin(valid_set), mode_value).otherwise(col(column)))\n",
    "\n",
    "print(5)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6f118c4f-907b-4423-8b5c-0994bb47046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the 'smoke' column with 0s and 1s based on a random number generator\n",
    "#df = df.withColumn('smoke', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "773fd6b0-937b-41b6-89e0-afe26a62680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted male smoking rates by age: {(18, 24): 6.874257425742575, (25, 44): 16.342574257425742, (45, 64): 19.325742574257426, (65, inf): 10.765346534653467}\n",
      "Female smoking rates by age: {(18, 24): 5.3, (25, 44): 12.6, (45, 64): 14.9, (65, inf): 8.3}\n"
     ]
    }
   ],
   "source": [
    "# Set the webpage URL for fetching data\n",
    "data_url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release\"\n",
    "# Send a GET request to the URL\n",
    "web_response = requests.get(data_url)\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "html_content = BeautifulSoup(web_response.content, 'html.parser')\n",
    "# Specify a key phrase from the chart caption to locate the right data\n",
    "search_caption = \"Proportion of people 15 years and over who were current daily smokers by age, 2011\"\n",
    "# Initialize variable to store the desired div\n",
    "target_div = None\n",
    "\n",
    "# Loop through all div elements with the specified class\n",
    "for container in html_content.find_all('div', {'class': 'chart-data-wrapper'}):\n",
    "    # Extract the caption text\n",
    "    chart_caption = container.find('pre', {'class': 'chart-caption'}).text\n",
    "    # Check if the specified caption part is in the extracted caption\n",
    "    if search_caption in chart_caption:\n",
    "        target_div = container\n",
    "        break\n",
    "\n",
    "# Parse and extract chart data from JSON format\n",
    "chart_data = json.loads(target_div.find('pre', {'class': 'chart-data'}).text)\n",
    "desired_values = chart_data[7]\n",
    "\n",
    "# Smoking rates by age group as extracted\n",
    "smoking_rates = [item for sublist in desired_values for item in sublist]\n",
    "\n",
    "# Define age bins corresponding to the age groups in the rate table\n",
    "bins = [0, 17, 24, 34, 44, 54, 64, 74, 120]\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Function to assign each age to an age group\n",
    "def assign_age_group(age):\n",
    "    for i, bin_end in enumerate(bins[1:]):\n",
    "        if age < bin_end:\n",
    "            return labels[i]\n",
    "    return labels[-1]\n",
    "\n",
    "assign_age_group_udf = udf(assign_age_group, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the age group column\n",
    "df = df.withColumn('age_group_ABS', assign_age_group_udf(col('age')))\n",
    "\n",
    "# Function to impute NaN based on smoking probability\n",
    "def impute_smoking(abs_smoke, age_group):\n",
    "    if abs_smoke is None:\n",
    "        rate = smoking_rates[int(age_group)]\n",
    "        return 1 if np.random.rand() < rate / 100 else 0\n",
    "    else:\n",
    "        return abs_smoke\n",
    "\n",
    "impute_smoking_udf = udf(impute_smoking, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the ABS smoke column\n",
    "df = df.withColumn('ABS_smoke', impute_smoking_udf(col('smoke'), col('age_group_ABS')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch data from CDC website\n",
    "source_url = \"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm\"\n",
    "server_response = requests.get(source_url)\n",
    "if server_response.status_code != 200:\n",
    "    print(\"Failed to retrieve data\")\n",
    "\n",
    "html_data = server_response.content\n",
    "selector = Selector(text=html_data)\n",
    "target_div = selector.xpath(\"//div[@class='row '][3]\")\n",
    "\n",
    "list_selector = target_div.xpath(\"//ul[@class='block-list']\")\n",
    "gender_data = list_selector[0].xpath(\".//li/text()\").getall()\n",
    "age_data = list_selector[1].xpath(\".//li/text()\").getall()\n",
    "\n",
    "male_rate = float(gender_data[0].split(\"(\")[1].split(\"%)\")[0])\n",
    "female_rate = float(gender_data[1].split(\"(\")[1].split(\"%)\")[0])\n",
    "\n",
    "age_rates = {}\n",
    "for item in age_data:\n",
    "    age_range = item.split(\"aged \")[1].split(\" years\")[0]\n",
    "    rate = float(item.split(\"(\")[1].split(\"%)\")[0])\n",
    "    if \"–\" in age_range:\n",
    "        age_limits = age_range.split(\"–\")\n",
    "        age_rates[(int(age_limits[0]), int(age_limits[1]))] = rate\n",
    "    else:\n",
    "        age_rates[(int(age_range), float('inf'))] = rate\n",
    "\n",
    "adjusted_male_rates = {key: value * (male_rate / female_rate) for key, value in age_rates.items()}\n",
    "\n",
    "print(\"Adjusted male smoking rates by age:\", adjusted_male_rates)\n",
    "print(\"Female smoking rates by age:\", age_rates)\n",
    "\n",
    "bins = [18, 24, 44, 64, float('inf')]\n",
    "labels = [(18, 24), (25, 44), (45, 64), (65, float('inf'))]\n",
    "\n",
    "# Function to assign each age to a CDC age group\n",
    "def assign_cdc_age_group(age):\n",
    "    for i, bin_end in enumerate(bins[1:]):\n",
    "        if age < bin_end:\n",
    "            return labels[i]\n",
    "    return labels[-1]\n",
    "\n",
    "assign_cdc_age_group_udf = udf(assign_cdc_age_group, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the CDC age group column\n",
    "df = df.withColumn('age_group_CDC', assign_cdc_age_group_udf(col('age')))\n",
    "\n",
    "\n",
    "# Function to impute smoking based on CDC data\n",
    "def impute_cdc_smoking(smoke, age_group, sex):\n",
    "    if smoke is None:\n",
    "        age_group = tuple(age_group)\n",
    "        if sex == 1:  # Male\n",
    "            rate = adjusted_male_rates.get(age_group, 0)\n",
    "            return 1 if np.random.rand() < rate / 100 else 0\n",
    "        else:  # Female\n",
    "            rate = age_rates.get(age_group, 0)\n",
    "            return 1 if np.random.rand() < rate / 100 else 0\n",
    "    else:\n",
    "        return smoke\n",
    "\n",
    "impute_cdc_smoking_udf = udf(impute_cdc_smoking, IntegerType())\n",
    "\n",
    "\n",
    "# Apply the UDF to create the CDC smoke column\n",
    "df = df.withColumn('CDC_smoke', impute_cdc_smoking_udf(col('smoke'), col('age_group_CDC'), col('sex')))\n",
    "\n",
    "\n",
    "# Function to impute the 'smoke' column based on ABS and CDC smoke columns\n",
    "def impute_smoke(smoke, abs_smoke, cdc_smoke):\n",
    "    if smoke is None:\n",
    "        if abs_smoke == 0 and cdc_smoke == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return smoke\n",
    "\n",
    "impute_smoke_udf = udf(impute_smoke, IntegerType())\n",
    "\n",
    "# Apply the UDF to update the 'smoke' column\n",
    "df = df.withColumn('smoke', impute_smoke_udf(col('smoke'), col('ABS_smoke'), col('CDC_smoke')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5c90f1b-79a8-49cb-a643-2328205c3ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "|age|sex|painloc|painexer| cp|trestbps|smoke|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "| 63|  1|      1|       1|  1|   145.0|    1|  1|   0|   0|  0|       0|   10.5|  150.0|    0|    2.3|    3|     0|\n",
      "| 67|  1|      1|       1|  4|   160.0|    0|  0|   1|   0|  0|       0|    9.5|  108.0|    1|    1.5|    2|     1|\n",
      "| 67|  1|      1|       1|  4|   120.0|    1|  0|   1|   0|  0|       0|    8.5|  129.0|    1|    2.6|    2|     1|\n",
      "| 37|  1|      1|       1|  3|   130.0|    0|  0|   1|   0|  0|       0|   13.0|  187.0|    0|    3.5|    3|     0|\n",
      "| 41|  0|      1|       1|  2|   130.0|    1|  0|   0|   0|  0|       0|    7.0|  172.0|    0|    1.4|    1|     0|\n",
      "| 56|  1|      1|       1|  2|   120.0|    0|  0|   0|   0|  0|       0|   11.3|  178.0|    0|    0.8|    1|     0|\n",
      "| 62|  0|      1|       1|  4|   140.0|    1|  0|   0|   0|  0|       0|    6.0|  160.0|    0|    3.6|    3|     1|\n",
      "| 57|  0|      1|       1|  4|   120.0|    0|  0|   0|   0|  0|       0|    9.0|  163.0|    1|    0.6|    1|     0|\n",
      "| 63|  1|      1|       1|  4|   130.0|    0|  0|   1|   1|  0|       0|    8.0|  147.0|    0|    1.4|    2|     1|\n",
      "| 53|  1|      1|       1|  4|   140.0|    0|  1|   1|   0|  0|       1|    5.5|  155.0|    1|    3.1|    3|     1|\n",
      "| 57|  1|      1|       1|  4|   140.0|    0|  0|   0|   0|  0|       0|    8.2|  148.0|    0|    0.4|    2|     0|\n",
      "| 56|  0|      1|       1|  2|   140.0|    1|  0|   1|   1|  0|       0|    4.5|  153.0|    0|    1.3|    2|     0|\n",
      "| 56|  1|      1|       1|  3|   130.0|    0|  1|   0|   0|  0|       0|   13.0|  142.0|    1|    0.6|    2|     1|\n",
      "| 44|  1|      1|       1|  2|   120.0|    1|  0|   1|   0|  0|       0|    9.3|  173.0|    0|    0.0|    1|     0|\n",
      "| 52|  1|      1|       1|  3|   172.0|    1|  1|   0|   0|  0|       0|   12.5|  162.0|    0|    0.5|    1|     0|\n",
      "| 57|  1|      1|       1|  3|   150.0|    0|  0|   0|   1|  0|       0|   11.0|  174.0|    0|    1.6|    1|     0|\n",
      "| 48|  1|      1|       1|  2|   110.0|    1|  0|   1|   0|  0|       0|    9.8|  168.0|    0|    1.0|    3|     1|\n",
      "| 54|  1|      1|       1|  4|   140.0|    1|  0|   0|   0|  0|       1|    7.8|  160.0|    0|    1.2|    1|     0|\n",
      "| 48|  0|      1|       1|  3|   130.0|    1|  0|   0|   0|  0|       0|   10.0|  139.0|    0|    0.2|    1|     0|\n",
      "| 49|  1|      1|       1|  2|   130.0|    1|  0|   0|   0|  0|       0|   12.0|  171.0|    0|    0.6|    1|     0|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__provides__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m nan_counts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect([count(when(isnan(c) \u001b[38;5;241m|\u001b[39m col(c)\u001b[38;5;241m.\u001b[39misNull(), c))\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Setting up the logistic regression with hyperparameter grid\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m log_reg \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m log_reg_param_grid \u001b[38;5;241m=\u001b[39m ParamGridBuilder() \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39maddGrid(log_reg\u001b[38;5;241m.\u001b[39mregParam, [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m]) \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Setting up cross-validation\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/classification.py:1317\u001b[0m, in \u001b[0;36mLogisticRegression.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, threshold, thresholds, probabilityCol, rawPredictionCol, standardization, weightCol, aggregationDepth, family, lowerBoundsOnCoefficients, upperBoundsOnCoefficients, lowerBoundsOnIntercepts, upperBoundsOnIntercepts, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     maxBlockSizeInMB: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1305\u001b[0m ):\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m             maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    If the threshold and thresholds Params are both set, they must be equivalent.\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1317\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.classification.LogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:49\u001b[0m, in \u001b[0;36mJavaWrapper.__init__\u001b[0;34m(self, java_obj)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_obj: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/classification.py:1008\u001b[0m, in \u001b[0;36m_LogisticRegressionParams.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any):\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_LogisticRegressionParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m   1010\u001b[0m         maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, family\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxBlockSizeInMB\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:151\u001b[0m, in \u001b[0;36mHasProbabilityCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasProbabilityCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(probabilityCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:512\u001b[0m, in \u001b[0;36mHasThresholds.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasThresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:174\u001b[0m, in \u001b[0;36mHasRawPredictionCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasRawPredictionCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(rawPredictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:105\u001b[0m, in \u001b[0;36mHasLabelCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasLabelCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:82\u001b[0m, in \u001b[0;36mHasFeaturesCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasFeaturesCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:128\u001b[0m, in \u001b[0;36mHasPredictionCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasPredictionCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:60\u001b[0m, in \u001b[0;36mHasRegParam.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasRegParam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:443\u001b[0m, in \u001b[0;36mHasElasticNetParam.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasElasticNetParam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(elasticNetParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:38\u001b[0m, in \u001b[0;36mHasMaxIter.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasMaxIter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:466\u001b[0m, in \u001b[0;36mHasFitIntercept.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasFitIntercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(fitIntercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:354\u001b[0m, in \u001b[0;36mHasTol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasTol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:489\u001b[0m, in \u001b[0;36mHasStandardization.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasStandardization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(standardization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:557\u001b[0m, in \u001b[0;36mHasWeightCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasWeightCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:624\u001b[0m, in \u001b[0;36mHasAggregationDepth.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasAggregationDepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(aggregationDepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:534\u001b[0m, in \u001b[0;36mHasThreshold.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasThreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:782\u001b[0m, in \u001b[0;36mHasMaxBlockSizeInMB.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasMaxBlockSizeInMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(maxBlockSizeInMB\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:269\u001b[0m, in \u001b[0;36mParams.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params: Optional[List[Param]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Copy the params from the class to the object\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36mParams._copy_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "\u001b[0;31mAttributeError\u001b[0m: __provides__"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "#df = df.drop('age_group_ABS', 'age_group_CDC') if 'age_group_ABS' in df.columns and 'age_group_CDC' in df.columns else df\n",
    "\n",
    "# Fill the 'smoke' column with 0s and 1s based on a random number generator\n",
    "df = df.withColumn('smoke', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Split the data into features and target\n",
    "target_column = 'target'\n",
    "feature_columns = [column for column in df.columns if column != target_column]\n",
    "\n",
    "# Split the data with stratification\n",
    "stratified_df = df.withColumn('rand', rand())\n",
    "train_df = stratified_df.where(col('rand') >= 0.1).drop('rand')\n",
    "test_df = stratified_df.where(col('rand') < 0.1).drop('rand')\n",
    "\n",
    "# Count NaNs in each column\n",
    "nan_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Setting up the logistic regression with hyperparameter grid\n",
    "log_reg = LogisticRegression(labelCol=target_column)\n",
    "log_reg_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(log_reg.regParam, [0.01, 0.1, 1, 10, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Setting up cross-validation\n",
    "crossval_log_reg = CrossValidator(estimator=log_reg,\n",
    "                                  estimatorParamMaps=log_reg_param_grid,\n",
    "                                  evaluator=MulticlassClassificationEvaluator(labelCol=target_column, metricName='accuracy'),\n",
    "                                  numFolds=5)\n",
    "\n",
    "# Fit logistic regression model\n",
    "log_reg_model = crossval_log_reg.fit(train_df)\n",
    "best_log_reg_model = log_reg_model.bestModel\n",
    "\n",
    "print(\"Best parameters for Logistic Regression:\", best_log_reg_model.extractParamMap())\n",
    "print(\"Cross-validated accuracy:\", log_reg_model.avgMetrics[0])\n",
    "\n",
    "# Setting up the random forest classifier with hyperparameter grid\n",
    "rf = RandomForestClassifier(labelCol=target_column)\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100, 200]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "# Setting up cross-validation\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=rf_param_grid,\n",
    "                             evaluator=MulticlassClassificationEvaluator(labelCol=target_column, metricName='accuracy'),\n",
    "                             numFolds=5)\n",
    "\n",
    "# Fit random forest model\n",
    "rf_model = crossval_rf.fit(train_df)\n",
    "best_rf_model = rf_model.bestModel\n",
    "\n",
    "print(\"Best parameters for Random Forest:\", best_rf_model.extractParamMap())\n",
    "print(\"Cross-validated accuracy:\", rf_model.avgMetrics[0])\n",
    "\n",
    "# Compare the performance and select the best model\n",
    "if log_reg_model.avgMetrics[0] > rf_model.avgMetrics[0]:\n",
    "    final_model = best_log_reg_model\n",
    "    print(\"Selected Logistic Regression as the final model.\")\n",
    "else:\n",
    "    final_model = best_rf_model\n",
    "    print(\"Selected Random Forest as the final model.\")\n",
    "\n",
    "# Final evaluation on the test data\n",
    "predictions = final_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "35b87372-38c2-461b-8094-83e8c475d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 06:40:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__provides__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance on the test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[141], line 36\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     34\u001b[0m imputers_string \u001b[38;5;241m=\u001b[39m [Imputer(inputCol\u001b[38;5;241m=\u001b[39mindexed_col, outputCol\u001b[38;5;241m=\u001b[39mimputed_col, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m indexed_col, imputed_col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(indexed_string_columns, imputed_string_columns)]\n\u001b[1;32m     35\u001b[0m imputed_numeric_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImputed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_features]\n\u001b[0;32m---> 36\u001b[0m imputer_numeric \u001b[38;5;241m=\u001b[39m \u001b[43mImputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimputed_numeric_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Assemble feature columns into a single feature vector\u001b[39;00m\n\u001b[1;32m     39\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(\n\u001b[1;32m     40\u001b[0m     inputCols\u001b[38;5;241m=\u001b[39mimputed_numeric_columns \u001b[38;5;241m+\u001b[39m imputed_string_columns,\n\u001b[1;32m     41\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/feature.py:2120\u001b[0m, in \u001b[0;36mImputer.__init__\u001b[0;34m(self, strategy, missingValue, inputCols, outputCols, inputCol, outputCol, relativeError)\u001b[0m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   2106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     relativeError: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   2115\u001b[0m ):\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;124;03m    __init__(self, \\\\*, strategy=\"mean\", missingValue=float(\"nan\"), inputCols=None, \\\u001b[39;00m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;124;03m             outputCols=None, inputCol=None, outputCol=None, relativeError=0.001):\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImputer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.feature.Imputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m   2122\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:49\u001b[0m, in \u001b[0;36mJavaWrapper.__init__\u001b[0;34m(self, java_obj)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_obj: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/feature.py:1943\u001b[0m, in \u001b[0;36m_ImputerParams.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any):\n\u001b[0;32m-> 1943\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ImputerParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, missingValue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m), relativeError\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:197\u001b[0m, in \u001b[0;36mHasInputCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasInputCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:219\u001b[0m, in \u001b[0;36mHasInputCols.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasInputCols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:241\u001b[0m, in \u001b[0;36mHasOutputCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasOutputCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:264\u001b[0m, in \u001b[0;36mHasOutputCols.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasOutputCols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:376\u001b[0m, in \u001b[0;36mHasRelativeError.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasRelativeError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(relativeError\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:269\u001b[0m, in \u001b[0;36mParams.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params: Optional[List[Param]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Copy the params from the class to the object\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36mParams._copy_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "\u001b[0;31mAttributeError\u001b[0m: __provides__"
     ]
    }
   ],
   "source": [
    "def pipeline(data):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"ModelTrainingWithoutVectorAssembler\").getOrCreate()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    data = data.drop('age_group_ABS', 'age_group_CDC') if 'age_group_ABS' in data.columns and 'age_group_CDC' in data.columns else data\n",
    "\n",
    "    # Fill the 'smoke' column with 0s and 1s based on a random number generator\n",
    "    data = data.withColumn('smoke', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "    # Define target and feature columns\n",
    "    target_column = 'target'\n",
    "    feature_columns = [column for column in data.columns if column != target_column]\n",
    "\n",
    "    # Splitting the data with stratification\n",
    "    stratified_data = data.withColumn('rand', rand())\n",
    "    train_data = stratified_data.where(col('rand') >= 0.1).drop('rand')\n",
    "    test_data = stratified_data.where(col('rand') < 0.1).drop('rand')\n",
    "\n",
    "    # Count NaNs in each column\n",
    "    nan_counts = data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns])\n",
    "    #nan_counts.show()\n",
    "\n",
    "    # Identify numeric and string features\n",
    "    numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, (DoubleType, FloatType, IntegerType, LongType))]\n",
    "    string_features = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "    # Index string features\n",
    "    indexed_string_columns = [f\"{col}_Index\" for col in string_features]\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=indexed_col, handleInvalid=\"keep\") for col, indexed_col in zip(string_features, indexed_string_columns)]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputed_string_columns = [f\"Imputed_{col}\" for col in indexed_string_columns]\n",
    "    imputers_string = [Imputer(inputCol=indexed_col, outputCol=imputed_col, strategy=\"mode\") for indexed_col, imputed_col in zip(indexed_string_columns, imputed_string_columns)]\n",
    "    imputed_numeric_columns = [f\"Imputed_{col}\" for col in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_numeric_columns, strategy=\"mean\")\n",
    "\n",
    "    # Assemble feature columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_numeric_columns + imputed_string_columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Define classifiers\n",
    "    log_reg = LogisticRegression(labelCol=target_column, featuresCol=\"features\")\n",
    "    rf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\")\n",
    "\n",
    "    # Set up the parameter grids\n",
    "    log_reg_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(log_reg.regParam, [0.01, 0.1, 1, 10, 100]) \\\n",
    "        .build()\n",
    "\n",
    "    rf_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 50, 100, 200]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10, 20, 30]) \\\n",
    "        .build()\n",
    "\n",
    "    # Set up the cross-validators\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName=\"accuracy\")\n",
    "\n",
    "    log_reg_cv = CrossValidator(\n",
    "        estimator=Pipeline(stages=indexers + imputers_string + [imputer_numeric, assembler, log_reg]),\n",
    "        estimatorParamMaps=log_reg_param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5\n",
    "    )\n",
    "\n",
    "    rf_cv = CrossValidator(\n",
    "        estimator=Pipeline(stages=indexers + imputers_string + [imputer_numeric, assembler, rf]),\n",
    "        estimatorParamMaps=rf_param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5\n",
    "    )\n",
    "\n",
    "    # Fit the models\n",
    "    log_reg_model = log_reg_cv.fit(train_data)\n",
    "    rf_model = rf_cv.fit(train_data)\n",
    "\n",
    "    # Compare the performance and select the best model\n",
    "    if log_reg_model.avgMetrics[0] > rf_model.avgMetrics[0]:\n",
    "        final_model = log_reg_model.bestModel\n",
    "        print(\"Selected Logistic Regression as the final model.\")\n",
    "    else:\n",
    "        final_model = rf_model.bestModel\n",
    "        print(\"Selected Random Forest as the final model.\")\n",
    "\n",
    "    # Final evaluation on the test data\n",
    "    predictions = final_model.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Performance on the test set:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60725b-70ec-4c14-bb09-ee9173ba2a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
