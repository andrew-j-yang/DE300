{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a00790-6811-467a-8738-c24e8c1f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean as _mean, lit\n",
    "from pyspark.sql.functions import col, lit, row_number, rand, when, isnan, count\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType, LongType, StringType\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "#from scrapy import Selector\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Heart Disease Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data/heart_disease.csv\", header = True, inferSchema = True)\n",
    "\n",
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f9c71b-402f-4ae0-9971-bb8c8294732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the desired columns\n",
    "selected_columns = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "\n",
    "# Selecting only the desired columns\n",
    "df = df.select(*selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e8a7cb-7ead-47f1-ac3e-35c3c5796230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "|age|sex|painloc|painexer| cp|trestbps|smoke|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "| 63|  1|      1|       1|  1|   145.0| null|  1|   0|   0|  0|       0|   10.5|  150.0|    0|    2.3|    3|     0|\n",
      "| 67|  1|      1|       1|  4|   160.0| null|  0|   1|   0|  0|       0|    9.5|  108.0|    1|    1.5|    2|     1|\n",
      "| 67|  1|      1|       1|  4|   120.0| null|  0|   1|   0|  0|       0|    8.5|  129.0|    1|    2.6|    2|     1|\n",
      "| 37|  1|      1|       1|  3|   130.0| null|  0|   1|   0|  0|       0|   13.0|  187.0|    0|    3.5|    3|     0|\n",
      "| 41|  0|      1|       1|  2|   130.0| null|  0|   0|   0|  0|       0|    7.0|  172.0|    0|    1.4|    1|     0|\n",
      "| 56|  1|      1|       1|  2|   120.0| null|  0|   0|   0|  0|       0|   11.3|  178.0|    0|    0.8|    1|     0|\n",
      "| 62|  0|      1|       1|  4|   140.0| null|  0|   0|   0|  0|       0|    6.0|  160.0|    0|    3.6|    3|     1|\n",
      "| 57|  0|      1|       1|  4|   120.0| null|  0|   0|   0|  0|       0|    9.0|  163.0|    1|    0.6|    1|     0|\n",
      "| 63|  1|      1|       1|  4|   130.0| null|  0|   1|   1|  0|       0|    8.0|  147.0|    0|    1.4|    2|     1|\n",
      "| 53|  1|      1|       1|  4|   140.0| null|  1|   1|   0|  0|       1|    5.5|  155.0|    1|    3.1|    3|     1|\n",
      "| 57|  1|      1|       1|  4|   140.0| null|  0|   0|   0|  0|       0|    8.2|  148.0|    0|    0.4|    2|     0|\n",
      "| 56|  0|      1|       1|  2|   140.0| null|  0|   1|   1|  0|       0|    4.5|  153.0|    0|    1.3|    2|     0|\n",
      "| 56|  1|      1|       1|  3|   130.0| null|  1|   0|   0|  0|       0|   13.0|  142.0|    1|    0.6|    2|     1|\n",
      "| 44|  1|      1|       1|  2|   120.0| null|  0|   1|   0|  0|       0|    9.3|  173.0|    0|    0.0|    1|     0|\n",
      "| 52|  1|      1|       1|  3|   172.0| null|  1|   0|   0|  0|       0|   12.5|  162.0|    0|    0.5|    1|     0|\n",
      "| 57|  1|      1|       1|  3|   150.0| null|  0|   0|   1|  0|       0|   11.0|  174.0|    0|    1.6|    1|     0|\n",
      "| 48|  1|      1|       1|  2|   110.0| null|  0|   1|   0|  0|       0|    9.8|  168.0|    0|    1.0|    3|     1|\n",
      "| 54|  1|      1|       1|  4|   140.0| null|  0|   0|   0|  0|       1|    7.8|  160.0|    0|    1.2|    1|     0|\n",
      "| 48|  0|      1|       1|  3|   130.0| null|  0|   0|   0|  0|       0|   10.0|  139.0|    0|    0.2|    1|     0|\n",
      "| 49|  1|      1|       1|  2|   130.0| null|  0|   0|   0|  0|       0|   12.0|  171.0|    0|    0.6|    1|     0|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_attributes = [\n",
    "    'painloc',      # Chest pain location (1 = substernal, 0 = otherwise)\n",
    "    'painexer',     # Whether pain is provoked by exertion (1 = yes, 0 = no)\n",
    "    'fbs',          # Fasting blood sugar > 120 mg/dL (1 = true, 0 = false)\n",
    "    'prop',         # Beta blocker used during exercise ECG (1 = yes, 0 = no)\n",
    "    'nitr',         # Nitrates used during exercise ECG (1 = yes, 0 = no)\n",
    "    'pro',          # Calcium channel blocker used during exercise ECG (1 = yes, 0 = no)\n",
    "    'diuretic',     # Diuretic used during exercise ECG (1 = yes, 0 = no)\n",
    "    'exang'\n",
    "]\n",
    "\n",
    "for column in binary_attributes:\n",
    "    # Calculate the mode of the binary column\n",
    "    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "    \n",
    "    # Replace non-binary and NaN values with the mode\n",
    "    df = df.withColumn(column, when((col(column).isNull()) | (~col(column).isin(0, 1)), mode_value).otherwise(col(column)))\n",
    "\n",
    "# Replace missing values in 'thaldur' column with the average of the column\n",
    "thaldur_average = df.agg(_mean(col('thaldur')).alias('mean')).first()['mean']\n",
    "df = df.withColumn('thaldur', when(col('thaldur').isNull(), thaldur_average).otherwise(col('thaldur')))\n",
    "\n",
    "# Replace missing values in 'thalach' column with the average of the column\n",
    "thalach_average = df.agg(_mean(col('thalach')).alias('mean')).first()['mean']\n",
    "df = df.withColumn('thalach', when(col('thalach').isNull(), thalach_average).otherwise(col('thalach')))\n",
    "\n",
    "# Replace missing values in 'trestbps' column with the average of the column\n",
    "trestbps_average = df.agg(_mean(col('trestbps')).alias('mean')).first()['mean']\n",
    "df = df.withColumn('trestbps', when(col('trestbps').isNull(), trestbps_average).otherwise(col('trestbps')))\n",
    "\n",
    "# Calculate the average of the 'oldpeak' column\n",
    "average_oldpeak = df.agg(_mean(col('oldpeak')).alias('mean')).first()['mean']\n",
    "\n",
    "# Replace missing values, values less than 0, and values greater than 4 with the average\n",
    "df = df.withColumn('oldpeak', when(col('oldpeak').isNull() | (col('oldpeak') < 0) | (col('oldpeak') > 4), average_oldpeak).otherwise(col('oldpeak')))\n",
    "\n",
    "valid_categories = {\n",
    "    'cp': {1, 2, 3, 4},\n",
    "    'slope': {1, 2, 3},\n",
    "}\n",
    "\n",
    "for column, valid_set in valid_categories.items():\n",
    "    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]\n",
    "    df = df.withColumn(column, when(~col(column).isin(valid_set), mode_value).otherwise(col(column)))\n",
    "\n",
    "print(5)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6f118c4f-907b-4423-8b5c-0994bb47046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the 'smoke' column with 0s and 1s based on a random number generator\n",
    "#df = df.withColumn('smoke', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "773fd6b0-937b-41b6-89e0-afe26a62680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted male smoking rates by age: {(18, 24): 6.874257425742575, (25, 44): 16.342574257425742, (45, 64): 19.325742574257426, (65, inf): 10.765346534653467}\n",
      "Female smoking rates by age: {(18, 24): 5.3, (25, 44): 12.6, (45, 64): 14.9, (65, inf): 8.3}\n"
     ]
    }
   ],
   "source": [
    "# Set the webpage URL for fetching data\n",
    "data_url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release\"\n",
    "# Send a GET request to the URL\n",
    "web_response = requests.get(data_url)\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "html_content = BeautifulSoup(web_response.content, 'html.parser')\n",
    "# Specify a key phrase from the chart caption to locate the right data\n",
    "search_caption = \"Proportion of people 15 years and over who were current daily smokers by age, 2011\"\n",
    "# Initialize variable to store the desired div\n",
    "target_div = None\n",
    "\n",
    "# Loop through all div elements with the specified class\n",
    "for container in html_content.find_all('div', {'class': 'chart-data-wrapper'}):\n",
    "    # Extract the caption text\n",
    "    chart_caption = container.find('pre', {'class': 'chart-caption'}).text\n",
    "    # Check if the specified caption part is in the extracted caption\n",
    "    if search_caption in chart_caption:\n",
    "        target_div = container\n",
    "        break\n",
    "\n",
    "# Parse and extract chart data from JSON format\n",
    "chart_data = json.loads(target_div.find('pre', {'class': 'chart-data'}).text)\n",
    "desired_values = chart_data[7]\n",
    "\n",
    "# Smoking rates by age group as extracted\n",
    "smoking_rates = [item for sublist in desired_values for item in sublist]\n",
    "\n",
    "# Define age bins corresponding to the age groups in the rate table\n",
    "bins = [0, 17, 24, 34, 44, 54, 64, 74, 120]\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Function to assign each age to an age group\n",
    "def assign_age_group(age):\n",
    "    for i, bin_end in enumerate(bins[1:]):\n",
    "        if age < bin_end:\n",
    "            return labels[i]\n",
    "    return labels[-1]\n",
    "\n",
    "assign_age_group_udf = udf(assign_age_group, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the age group column\n",
    "df = df.withColumn('age_group_ABS', assign_age_group_udf(col('age')))\n",
    "\n",
    "# Function to impute NaN based on smoking probability\n",
    "def impute_smoking(abs_smoke, age_group):\n",
    "    if abs_smoke is None:\n",
    "        rate = smoking_rates[int(age_group)]\n",
    "        return 1 if np.random.rand() < rate / 100 else 0\n",
    "    else:\n",
    "        return abs_smoke\n",
    "\n",
    "impute_smoking_udf = udf(impute_smoking, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the ABS smoke column\n",
    "df = df.withColumn('ABS_smoke', impute_smoking_udf(col('smoke'), col('age_group_ABS')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch data from CDC website\n",
    "source_url = \"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm\"\n",
    "server_response = requests.get(source_url)\n",
    "if server_response.status_code != 200:\n",
    "    print(\"Failed to retrieve data\")\n",
    "\n",
    "html_data = server_response.content\n",
    "selector = Selector(text=html_data)\n",
    "target_div = selector.xpath(\"//div[@class='row '][3]\")\n",
    "\n",
    "list_selector = target_div.xpath(\"//ul[@class='block-list']\")\n",
    "gender_data = list_selector[0].xpath(\".//li/text()\").getall()\n",
    "age_data = list_selector[1].xpath(\".//li/text()\").getall()\n",
    "\n",
    "male_rate = float(gender_data[0].split(\"(\")[1].split(\"%)\")[0])\n",
    "female_rate = float(gender_data[1].split(\"(\")[1].split(\"%)\")[0])\n",
    "\n",
    "age_rates = {}\n",
    "for item in age_data:\n",
    "    age_range = item.split(\"aged \")[1].split(\" years\")[0]\n",
    "    rate = float(item.split(\"(\")[1].split(\"%)\")[0])\n",
    "    if \"–\" in age_range:\n",
    "        age_limits = age_range.split(\"–\")\n",
    "        age_rates[(int(age_limits[0]), int(age_limits[1]))] = rate\n",
    "    else:\n",
    "        age_rates[(int(age_range), float('inf'))] = rate\n",
    "\n",
    "adjusted_male_rates = {key: value * (male_rate / female_rate) for key, value in age_rates.items()}\n",
    "\n",
    "print(\"Adjusted male smoking rates by age:\", adjusted_male_rates)\n",
    "print(\"Female smoking rates by age:\", age_rates)\n",
    "\n",
    "bins = [18, 24, 44, 64, float('inf')]\n",
    "labels = [(18, 24), (25, 44), (45, 64), (65, float('inf'))]\n",
    "\n",
    "# Function to assign each age to a CDC age group\n",
    "def assign_cdc_age_group(age):\n",
    "    for i, bin_end in enumerate(bins[1:]):\n",
    "        if age < bin_end:\n",
    "            return labels[i]\n",
    "    return labels[-1]\n",
    "\n",
    "assign_cdc_age_group_udf = udf(assign_cdc_age_group, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the CDC age group column\n",
    "df = df.withColumn('age_group_CDC', assign_cdc_age_group_udf(col('age')))\n",
    "\n",
    "\n",
    "# Function to impute smoking based on CDC data\n",
    "def impute_cdc_smoking(smoke, age_group, sex):\n",
    "    if smoke is None:\n",
    "        age_group = tuple(age_group)\n",
    "        if sex == 1:  # Male\n",
    "            rate = adjusted_male_rates.get(age_group, 0)\n",
    "            return 1 if np.random.rand() < rate / 100 else 0\n",
    "        else:  # Female\n",
    "            rate = age_rates.get(age_group, 0)\n",
    "            return 1 if np.random.rand() < rate / 100 else 0\n",
    "    else:\n",
    "        return smoke\n",
    "\n",
    "impute_cdc_smoking_udf = udf(impute_cdc_smoking, IntegerType())\n",
    "\n",
    "\n",
    "# Apply the UDF to create the CDC smoke column\n",
    "df = df.withColumn('CDC_smoke', impute_cdc_smoking_udf(col('smoke'), col('age_group_CDC'), col('sex')))\n",
    "\n",
    "\n",
    "# Function to impute the 'smoke' column based on ABS and CDC smoke columns\n",
    "def impute_smoke(smoke, abs_smoke, cdc_smoke):\n",
    "    if smoke is None:\n",
    "        if abs_smoke == 0 and cdc_smoke == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return smoke\n",
    "\n",
    "impute_smoke_udf = udf(impute_smoke, IntegerType())\n",
    "\n",
    "# Apply the UDF to update the 'smoke' column\n",
    "df = df.withColumn('smoke', impute_smoke_udf(col('smoke'), col('ABS_smoke'), col('CDC_smoke')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5c90f1b-79a8-49cb-a643-2328205c3ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "|age|sex|painloc|painexer| cp|trestbps|smoke|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "| 63|  1|      1|       1|  1|   145.0|    1|  1|   0|   0|  0|       0|   10.5|  150.0|    0|    2.3|    3|     0|\n",
      "| 67|  1|      1|       1|  4|   160.0|    1|  0|   1|   0|  0|       0|    9.5|  108.0|    1|    1.5|    2|     1|\n",
      "| 67|  1|      1|       1|  4|   120.0|    0|  0|   1|   0|  0|       0|    8.5|  129.0|    1|    2.6|    2|     1|\n",
      "| 37|  1|      1|       1|  3|   130.0|    0|  0|   1|   0|  0|       0|   13.0|  187.0|    0|    3.5|    3|     0|\n",
      "| 41|  0|      1|       1|  2|   130.0|    1|  0|   0|   0|  0|       0|    7.0|  172.0|    0|    1.4|    1|     0|\n",
      "| 56|  1|      1|       1|  2|   120.0|    1|  0|   0|   0|  0|       0|   11.3|  178.0|    0|    0.8|    1|     0|\n",
      "| 62|  0|      1|       1|  4|   140.0|    1|  0|   0|   0|  0|       0|    6.0|  160.0|    0|    3.6|    3|     1|\n",
      "| 57|  0|      1|       1|  4|   120.0|    0|  0|   0|   0|  0|       0|    9.0|  163.0|    1|    0.6|    1|     0|\n",
      "| 63|  1|      1|       1|  4|   130.0|    0|  0|   1|   1|  0|       0|    8.0|  147.0|    0|    1.4|    2|     1|\n",
      "| 53|  1|      1|       1|  4|   140.0|    0|  1|   1|   0|  0|       1|    5.5|  155.0|    1|    3.1|    3|     1|\n",
      "| 57|  1|      1|       1|  4|   140.0|    1|  0|   0|   0|  0|       0|    8.2|  148.0|    0|    0.4|    2|     0|\n",
      "| 56|  0|      1|       1|  2|   140.0|    1|  0|   1|   1|  0|       0|    4.5|  153.0|    0|    1.3|    2|     0|\n",
      "| 56|  1|      1|       1|  3|   130.0|    1|  1|   0|   0|  0|       0|   13.0|  142.0|    1|    0.6|    2|     1|\n",
      "| 44|  1|      1|       1|  2|   120.0|    0|  0|   1|   0|  0|       0|    9.3|  173.0|    0|    0.0|    1|     0|\n",
      "| 52|  1|      1|       1|  3|   172.0|    0|  1|   0|   0|  0|       0|   12.5|  162.0|    0|    0.5|    1|     0|\n",
      "| 57|  1|      1|       1|  3|   150.0|    1|  0|   0|   1|  0|       0|   11.0|  174.0|    0|    1.6|    1|     0|\n",
      "| 48|  1|      1|       1|  2|   110.0|    1|  0|   1|   0|  0|       0|    9.8|  168.0|    0|    1.0|    3|     1|\n",
      "| 54|  1|      1|       1|  4|   140.0|    1|  0|   0|   0|  0|       1|    7.8|  160.0|    0|    1.2|    1|     0|\n",
      "| 48|  0|      1|       1|  3|   130.0|    0|  0|   0|   0|  0|       0|   10.0|  139.0|    0|    0.2|    1|     0|\n",
      "| 49|  1|      1|       1|  2|   130.0|    1|  0|   0|   0|  0|       0|   12.0|  171.0|    0|    0.6|    1|     0|\n",
      "+---+---+-------+--------+---+--------+-----+---+----+----+---+--------+-------+-------+-----+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: age, sex, painloc, painexer, cp, trestbps, smoke, fbs, prop, nitr, pro, diuretic, thaldur, thalach, exang, oldpeak, slope, target, CrossValidator_845d825c8c9d_rand",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     28\u001b[0m crossval_log_reg \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mlog_reg,\n\u001b[1;32m     29\u001b[0m                                   estimatorParamMaps\u001b[38;5;241m=\u001b[39mlog_reg_param_grid,\n\u001b[1;32m     30\u001b[0m                                   evaluator\u001b[38;5;241m=\u001b[39mMulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39mtarget_column, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     31\u001b[0m                                   numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Fit logistic regression model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m log_reg_model \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval_log_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m best_log_reg_model \u001b[38;5;241m=\u001b[39m log_reg_model\u001b[38;5;241m.\u001b[39mbestModel\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for Logistic Regression:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_log_reg_model\u001b[38;5;241m.\u001b[39mextractParamMap())\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/util.py:337\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    336\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: age, sex, painloc, painexer, cp, trestbps, smoke, fbs, prop, nitr, pro, diuretic, thaldur, thalach, exang, oldpeak, slope, target, CrossValidator_845d825c8c9d_rand"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "#df = df.drop('age_group_ABS', 'age_group_CDC') if 'age_group_ABS' in df.columns and 'age_group_CDC' in df.columns else df\n",
    "\n",
    "# Fill the 'smoke' column with 0s and 1s based on a random number generator\n",
    "df = df.withColumn('smoke', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Split the data into features and target\n",
    "target_column = 'target'\n",
    "feature_columns = [column for column in df.columns if column != target_column]\n",
    "\n",
    "# Split the data with stratification\n",
    "stratified_df = df.withColumn('rand', rand())\n",
    "train_df = stratified_df.where(col('rand') >= 0.1).drop('rand')\n",
    "test_df = stratified_df.where(col('rand') < 0.1).drop('rand')\n",
    "\n",
    "# Count NaNs in each column\n",
    "nan_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Setting up the logistic regression with hyperparameter grid\n",
    "log_reg = LogisticRegression(labelCol=target_column)\n",
    "log_reg_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(log_reg.regParam, [0.01, 0.1, 1, 10, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Setting up cross-validation\n",
    "crossval_log_reg = CrossValidator(estimator=log_reg,\n",
    "                                  estimatorParamMaps=log_reg_param_grid,\n",
    "                                  evaluator=MulticlassClassificationEvaluator(labelCol=target_column, metricName='accuracy'),\n",
    "                                  numFolds=5)\n",
    "\n",
    "# Fit logistic regression model\n",
    "log_reg_model = crossval_log_reg.fit(train_df)\n",
    "best_log_reg_model = log_reg_model.bestModel\n",
    "\n",
    "print(\"Best parameters for Logistic Regression:\", best_log_reg_model.extractParamMap())\n",
    "print(\"Cross-validated accuracy:\", log_reg_model.avgMetrics[0])\n",
    "\n",
    "# Setting up the random forest classifier with hyperparameter grid\n",
    "rf = RandomForestClassifier(labelCol=target_column)\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100, 200]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "# Setting up cross-validation\n",
    "crossval_rf = CrossValidator(estimator=rf,\n",
    "                             estimatorParamMaps=rf_param_grid,\n",
    "                             evaluator=MulticlassClassificationEvaluator(labelCol=target_column, metricName='accuracy'),\n",
    "                             numFolds=5)\n",
    "\n",
    "# Fit random forest model\n",
    "rf_model = crossval_rf.fit(train_df)\n",
    "best_rf_model = rf_model.bestModel\n",
    "\n",
    "print(\"Best parameters for Random Forest:\", best_rf_model.extractParamMap())\n",
    "print(\"Cross-validated accuracy:\", rf_model.avgMetrics[0])\n",
    "\n",
    "# Compare the performance and select the best model\n",
    "if log_reg_model.avgMetrics[0] > rf_model.avgMetrics[0]:\n",
    "    final_model = best_log_reg_model\n",
    "    print(\"Selected Logistic Regression as the final model.\")\n",
    "else:\n",
    "    final_model = best_rf_model\n",
    "    print(\"Selected Random Forest as the final model.\")\n",
    "\n",
    "# Final evaluation on the test data\n",
    "predictions = final_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Performance on the test set:\")\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "35b87372-38c2-461b-8094-83e8c475d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/22 06:40:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__provides__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance on the test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[141], line 36\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     34\u001b[0m imputers_string \u001b[38;5;241m=\u001b[39m [Imputer(inputCol\u001b[38;5;241m=\u001b[39mindexed_col, outputCol\u001b[38;5;241m=\u001b[39mimputed_col, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m indexed_col, imputed_col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(indexed_string_columns, imputed_string_columns)]\n\u001b[1;32m     35\u001b[0m imputed_numeric_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImputed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_features]\n\u001b[0;32m---> 36\u001b[0m imputer_numeric \u001b[38;5;241m=\u001b[39m \u001b[43mImputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputCols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimputed_numeric_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Assemble feature columns into a single feature vector\u001b[39;00m\n\u001b[1;32m     39\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(\n\u001b[1;32m     40\u001b[0m     inputCols\u001b[38;5;241m=\u001b[39mimputed_numeric_columns \u001b[38;5;241m+\u001b[39m imputed_string_columns,\n\u001b[1;32m     41\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/feature.py:2120\u001b[0m, in \u001b[0;36mImputer.__init__\u001b[0;34m(self, strategy, missingValue, inputCols, outputCols, inputCol, outputCol, relativeError)\u001b[0m\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   2106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     relativeError: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   2115\u001b[0m ):\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;124;03m    __init__(self, \\\\*, strategy=\"mean\", missingValue=float(\"nan\"), inputCols=None, \\\u001b[39;00m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;124;03m             outputCols=None, inputCol=None, outputCol=None, relativeError=0.001):\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImputer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.feature.Imputer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m   2122\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:49\u001b[0m, in \u001b[0;36mJavaWrapper.__init__\u001b[0;34m(self, java_obj)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_obj: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/feature.py:1943\u001b[0m, in \u001b[0;36m_ImputerParams.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any):\n\u001b[0;32m-> 1943\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_ImputerParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, missingValue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m), relativeError\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:197\u001b[0m, in \u001b[0;36mHasInputCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasInputCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:219\u001b[0m, in \u001b[0;36mHasInputCols.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasInputCols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:241\u001b[0m, in \u001b[0;36mHasOutputCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasOutputCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:264\u001b[0m, in \u001b[0;36mHasOutputCols.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasOutputCols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:376\u001b[0m, in \u001b[0;36mHasRelativeError.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasRelativeError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(relativeError\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:269\u001b[0m, in \u001b[0;36mParams.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params: Optional[List[Param]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Copy the params from the class to the object\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36mParams._copy_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "\u001b[0;31mAttributeError\u001b[0m: __provides__"
     ]
    }
   ],
   "source": [
    "def pipeline(data):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"ModelTrainingWithoutVectorAssembler\").getOrCreate()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    data = data.drop('age_group_ABS', 'age_group_CDC') if 'age_group_ABS' in data.columns and 'age_group_CDC' in data.columns else data\n",
    "\n",
    "    # Fill the 'smoke' column with 0s and 1s based on a random number generator\n",
    "    data = data.withColumn('smoke', when(rand() > 0.5, 1).otherwise(0))\n",
    "\n",
    "    # Define target and feature columns\n",
    "    target_column = 'target'\n",
    "    feature_columns = [column for column in data.columns if column != target_column]\n",
    "\n",
    "    # Splitting the data with stratification\n",
    "    stratified_data = data.withColumn('rand', rand())\n",
    "    train_data = stratified_data.where(col('rand') >= 0.1).drop('rand')\n",
    "    test_data = stratified_data.where(col('rand') < 0.1).drop('rand')\n",
    "\n",
    "    # Count NaNs in each column\n",
    "    nan_counts = data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns])\n",
    "    #nan_counts.show()\n",
    "\n",
    "    # Identify numeric and string features\n",
    "    numeric_features = [f.name for f in data.schema.fields if isinstance(f.dataType, (DoubleType, FloatType, IntegerType, LongType))]\n",
    "    string_features = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "    # Index string features\n",
    "    indexed_string_columns = [f\"{col}_Index\" for col in string_features]\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=indexed_col, handleInvalid=\"keep\") for col, indexed_col in zip(string_features, indexed_string_columns)]\n",
    "\n",
    "    # Impute missing values\n",
    "    imputed_string_columns = [f\"Imputed_{col}\" for col in indexed_string_columns]\n",
    "    imputers_string = [Imputer(inputCol=indexed_col, outputCol=imputed_col, strategy=\"mode\") for indexed_col, imputed_col in zip(indexed_string_columns, imputed_string_columns)]\n",
    "    imputed_numeric_columns = [f\"Imputed_{col}\" for col in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_numeric_columns, strategy=\"mean\")\n",
    "\n",
    "    # Assemble feature columns into a single feature vector\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_numeric_columns + imputed_string_columns,\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    # Define classifiers\n",
    "    log_reg = LogisticRegression(labelCol=target_column, featuresCol=\"features\")\n",
    "    rf = RandomForestClassifier(labelCol=target_column, featuresCol=\"features\")\n",
    "\n",
    "    # Set up the parameter grids\n",
    "    log_reg_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(log_reg.regParam, [0.01, 0.1, 1, 10, 100]) \\\n",
    "        .build()\n",
    "\n",
    "    rf_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 50, 100, 200]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10, 20, 30]) \\\n",
    "        .build()\n",
    "\n",
    "    # Set up the cross-validators\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName=\"accuracy\")\n",
    "\n",
    "    log_reg_cv = CrossValidator(\n",
    "        estimator=Pipeline(stages=indexers + imputers_string + [imputer_numeric, assembler, log_reg]),\n",
    "        estimatorParamMaps=log_reg_param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5\n",
    "    )\n",
    "\n",
    "    rf_cv = CrossValidator(\n",
    "        estimator=Pipeline(stages=indexers + imputers_string + [imputer_numeric, assembler, rf]),\n",
    "        estimatorParamMaps=rf_param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=5\n",
    "    )\n",
    "\n",
    "    # Fit the models\n",
    "    log_reg_model = log_reg_cv.fit(train_data)\n",
    "    rf_model = rf_cv.fit(train_data)\n",
    "\n",
    "    # Compare the performance and select the best model\n",
    "    if log_reg_model.avgMetrics[0] > rf_model.avgMetrics[0]:\n",
    "        final_model = log_reg_model.bestModel\n",
    "        print(\"Selected Logistic Regression as the final model.\")\n",
    "    else:\n",
    "        final_model = rf_model.bestModel\n",
    "        print(\"Selected Random Forest as the final model.\")\n",
    "\n",
    "    # Final evaluation on the test data\n",
    "    predictions = final_model.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(\"Performance on the test set:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60725b-70ec-4c14-bb09-ee9173ba2a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
