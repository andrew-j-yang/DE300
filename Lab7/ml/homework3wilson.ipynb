{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9466b88b-2350-492b-aa7e-c58c4114fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, mean, lit, udf\n",
    "from pyspark.sql.types import FloatType, IntegerType, DoubleType, ArrayType\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from scrapy import Selector\n",
    "import random\n",
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bfe2fc-b82c-43ce-bb7b-e486a40f00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/22 07:19:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__provides__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 236\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m--> 236\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 190\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mfit(train)\n\u001b[1;32m    192\u001b[0m lr_predictions \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/classification.py:1317\u001b[0m, in \u001b[0;36mLogisticRegression.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, threshold, thresholds, probabilityCol, rawPredictionCol, standardization, weightCol, aggregationDepth, family, lowerBoundsOnCoefficients, upperBoundsOnCoefficients, lowerBoundsOnIntercepts, upperBoundsOnIntercepts, maxBlockSizeInMB)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     maxBlockSizeInMB: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1305\u001b[0m ):\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m             maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    If the threshold and thresholds Params are both set, they must be equivalent.\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1317\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.classification.LogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/wrapper.py:49\u001b[0m, in \u001b[0;36mJavaWrapper.__init__\u001b[0;34m(self, java_obj)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, java_obj: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mJavaWrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m java_obj\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/classification.py:1008\u001b[0m, in \u001b[0;36m_LogisticRegressionParams.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any):\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_LogisticRegressionParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m   1010\u001b[0m         maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, family\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxBlockSizeInMB\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:151\u001b[0m, in \u001b[0;36mHasProbabilityCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasProbabilityCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(probabilityCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:512\u001b[0m, in \u001b[0;36mHasThresholds.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasThresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:174\u001b[0m, in \u001b[0;36mHasRawPredictionCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasRawPredictionCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(rawPredictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:105\u001b[0m, in \u001b[0;36mHasLabelCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasLabelCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:82\u001b[0m, in \u001b[0;36mHasFeaturesCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasFeaturesCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:128\u001b[0m, in \u001b[0;36mHasPredictionCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasPredictionCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:60\u001b[0m, in \u001b[0;36mHasRegParam.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasRegParam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:443\u001b[0m, in \u001b[0;36mHasElasticNetParam.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasElasticNetParam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(elasticNetParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:38\u001b[0m, in \u001b[0;36mHasMaxIter.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasMaxIter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:466\u001b[0m, in \u001b[0;36mHasFitIntercept.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasFitIntercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(fitIntercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:354\u001b[0m, in \u001b[0;36mHasTol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasTol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:489\u001b[0m, in \u001b[0;36mHasStandardization.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasStandardization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(standardization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:557\u001b[0m, in \u001b[0;36mHasWeightCol.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasWeightCol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:624\u001b[0m, in \u001b[0;36mHasAggregationDepth.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasAggregationDepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(aggregationDepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:534\u001b[0m, in \u001b[0;36mHasThreshold.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasThreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/shared.py:782\u001b[0m, in \u001b[0;36mHasMaxBlockSizeInMB.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHasMaxBlockSizeInMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(maxBlockSizeInMB\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:269\u001b[0m, in \u001b[0;36mParams.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params: Optional[List[Param]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Copy the params from the class to the object\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36mParams._copy_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "File \u001b[0;32m/tmp/demos/lib/python3.10/site-packages/pyspark/ml/param/__init__.py:276\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mCopy all params defined on the class to current object.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m src_name_attrs \u001b[38;5;241m=\u001b[39m [(x, \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mcls\u001b[39m)]\n\u001b[1;32m    277\u001b[0m src_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m nameAttr: \u001b[38;5;28misinstance\u001b[39m(nameAttr[\u001b[38;5;241m1\u001b[39m], Param), src_name_attrs))\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m src_params:\n",
      "\u001b[0;31mAttributeError\u001b[0m: __provides__"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"HeartDiseasePrediction\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "    # In[45]:\n",
    "\n",
    "    #df = spark.read.csv(\"s3://hw3wilson/data/heart_disease.csv\", header=True, inferSchema=True)\n",
    "    df = spark.read.csv(\"data/heart_disease.csv\", header = True, inferSchema = True)\n",
    "\n",
    "\n",
    "    # In[46]:\n",
    "\n",
    "\n",
    "    retain = [\n",
    "        'age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke',\n",
    "        'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang',\n",
    "        'oldpeak', 'slope'\n",
    "    ]\n",
    "    selected_columns_with_target = retain + ['target']\n",
    "    df = df.select(*selected_columns_with_target)\n",
    "\n",
    "    # Replace NaN in 'painloc' and 'painexer' with their modes\n",
    "    painloc_mode = df.groupBy('painloc').count().orderBy('count', ascending=False).first()['painloc']\n",
    "    painexer_mode = df.groupBy('painexer').count().orderBy('count', ascending=False).first()['painexer']\n",
    "    df = df.fillna({'painloc': painloc_mode, 'painexer': painexer_mode})\n",
    "\n",
    "    # Replace values in 'trestbps' below 100 with the mean of values above 100\n",
    "    mean_above_100 = df.filter(col('trestbps') > 100).select(mean('trestbps')).collect()[0][0]\n",
    "    df = df.withColumn('trestbps', when((col('trestbps') < 100) | isnan(col('trestbps')), mean_above_100).otherwise(col('trestbps')))\n",
    "\n",
    "    # Replace values in 'oldpeak' below 0 or above 4 with the mean of values between 0 and 4\n",
    "    mean_between_0_and_4 = df.filter((col('oldpeak') >= 0) & (col('oldpeak') <= 4)).select(mean('oldpeak')).collect()[0][0]\n",
    "    df = df.withColumn('oldpeak', when((col('oldpeak') < 0) | (col('oldpeak') > 4) | isnan(col('oldpeak')), mean_between_0_and_4).otherwise(col('oldpeak')))\n",
    "\n",
    "    # Replace NaN in 'thaldur' and 'thalach' with their means\n",
    "    mean_thaldur = df.select(mean('thaldur')).collect()[0][0]\n",
    "    mean_thalach = df.select(mean('thalach')).collect()[0][0]\n",
    "    df = df.fillna({'thaldur': mean_thaldur, 'thalach': mean_thalach})\n",
    "\n",
    "    # Replace values greater than 1 in specified columns with NaN and fill NaN with mode\n",
    "    columns_to_replace = ['fbs', 'prop', 'nitr', 'pro', 'diuretic']\n",
    "    for column in columns_to_replace:\n",
    "        mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[column]\n",
    "        df = df.withColumn(column, when(col(column) > 1, lit(None)).otherwise(col(column)))\n",
    "        df = df.fillna({column: mode_value})\n",
    "\n",
    "    exang_mode = df.groupBy('exang').count().orderBy('count', ascending=False).first()['exang']\n",
    "    slope_mode = df.groupBy('slope').count().orderBy('count', ascending=False).first()['slope']\n",
    "    df = df.fillna({'exang': exang_mode, 'slope': slope_mode})\n",
    "\n",
    "\n",
    "    # In[47]:\n",
    "\n",
    "\n",
    "    df = df.withColumn(\"source_1\", df[\"smoke\"]).withColumn(\"source_2\", df[\"smoke\"])\n",
    "\n",
    "    # Source 1: Scrape data and create the smoking dictionary\n",
    "    url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    part_of_caption = \"Proportion of people 15 years and over who were current daily smokers by age, 2011\"  # replace with your caption\n",
    "    div = None\n",
    "\n",
    "    for d in soup.find_all('div', {'class': 'chart-data-wrapper'}):\n",
    "        caption = d.find('pre', {'class': 'chart-caption'}).text\n",
    "        if part_of_caption in caption:\n",
    "            div = d\n",
    "            break\n",
    "\n",
    "    # Extract chart data\n",
    "    data = json.loads(div.find('pre', {'class': 'chart-data'}).text)\n",
    "    smoking_2022 = np.array(data[7]).flatten() / 100\n",
    "    age_dict = {\n",
    "        (15, 17): smoking_2022[0],\n",
    "        (18, 24): smoking_2022[1],\n",
    "        (25, 34): smoking_2022[2],\n",
    "        (35, 44): smoking_2022[3],\n",
    "        (45, 54): smoking_2022[4],\n",
    "        (55, 64): smoking_2022[5],\n",
    "        (65, 74): smoking_2022[6],\n",
    "        (75, 1000): smoking_2022[7]\n",
    "    }\n",
    "\n",
    "    # Impute missing values in 'source_1'\n",
    "    for age_range, rate in age_dict.items():\n",
    "        df = df.withColumn(\n",
    "            \"source_1\",\n",
    "            when(\n",
    "                (isnan(col(\"source_1\")) | (col(\"source_1\").isNull())) &\n",
    "                (col(\"age\").between(age_range[0], age_range[1])),\n",
    "                when(\n",
    "                    lit(np.random.rand()) <= rate, lit(1.0)\n",
    "                ).otherwise(lit(0.0))\n",
    "            ).otherwise(col(\"source_1\"))\n",
    "        )\n",
    "\n",
    "    tobacco_data_url = \"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm\"\n",
    "    response = requests.get(tobacco_data_url)\n",
    "    html_content = response.content\n",
    "    selector = Selector(text=html_content)\n",
    "\n",
    "    row_section = selector.xpath(\"//div[@class='row '][3]\")\n",
    "    unordered_lists = row_section.xpath(\"//ul[@class='block-list']\")\n",
    "\n",
    "    sex_rates = unordered_lists[0].xpath(\".//li/text()\").extract()  # Smoking rates by sex\n",
    "    age_rates = unordered_lists[1].xpath(\".//li/text()\").extract()  # Smoking rates by age group\n",
    "\n",
    "    male_smoking_rate = float(re.search(r\"\\((\\d+(\\.\\d+)?)%\", sex_rates[0]).group(1))\n",
    "    female_smoking_rate = float(re.search(r\"\\((\\d+(\\.\\d+)?)%\", sex_rates[1]).group(1))\n",
    "\n",
    "    age_based_rates = {}\n",
    "    for rate_text in age_rates:\n",
    "        age_info = re.search(r\"aged (\\d+–\\d+|\\d+)\", rate_text).group(1)\n",
    "        smoking_percentage = float(re.search(r\"\\((\\d+(\\.\\d+)?)%\", rate_text).group(1))\n",
    "\n",
    "        if \"–\" in age_info:\n",
    "            age_range = list(map(int, age_info.split(\"–\")))\n",
    "            age_based_rates[(age_range[0], age_range[1])] = smoking_percentage\n",
    "        else:\n",
    "            age_based_rates[(int(age_info), float('inf'))] = smoking_percentage\n",
    "\n",
    "    male_adjusted_rates = {\n",
    "        age_range: rate * (male_smoking_rate / female_smoking_rate)\n",
    "        for age_range, rate in age_based_rates.items()\n",
    "    }\n",
    "\n",
    "    female_rates = {key: value / 100 for key, value in age_based_rates.items()}\n",
    "    male_rates = {key: value / 100 for key, value in male_adjusted_rates.items()}\n",
    "\n",
    "    for age_range, rate in male_rates.items():\n",
    "        df = df.withColumn(\n",
    "            \"source_2\",\n",
    "            when(\n",
    "                (isnan(col(\"source_2\")) | (col(\"source_2\").isNull())) &\n",
    "                (col(\"age\").between(age_range[0], age_range[1])) &\n",
    "                (col(\"sex\") == 1),\n",
    "                when(\n",
    "                    lit(np.random.rand()) <= rate, lit(1.0)\n",
    "                ).otherwise(lit(0.0))\n",
    "            ).otherwise(col(\"source_2\"))\n",
    "        )\n",
    "\n",
    "    for age_range, rate in female_rates.items():\n",
    "        df = df.withColumn(\n",
    "            \"source_2\",\n",
    "            when(\n",
    "                (isnan(col(\"source_2\")) | (col(\"source_2\").isNull())) &\n",
    "                (col(\"age\").between(age_range[0], age_range[1])) &\n",
    "                (col(\"sex\") == 0),\n",
    "                when(\n",
    "                    lit(np.random.rand()) <= rate, lit(1.0)\n",
    "                ).otherwise(lit(0.0))\n",
    "            ).otherwise(col(\"source_2\"))\n",
    "        )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"smoke\",\n",
    "        when(\n",
    "            (isnan(col(\"smoke\")) | (col(\"smoke\").isNull())),\n",
    "            when(\n",
    "                (col(\"source_1\") + col(\"source_2\")) >= 1, lit(1.0)\n",
    "            ).otherwise(lit(0.0))\n",
    "        ).otherwise(col(\"smoke\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    # In[52]:\n",
    "\n",
    "\n",
    "    feature_columns = retain + ['source_1', 'source_2']\n",
    "\n",
    "    def create_feature_vector(*cols):\n",
    "        return [float(c) for c in cols]\n",
    "\n",
    "    create_feature_vector_udf = udf(create_feature_vector)\n",
    "\n",
    "    df = df.withColumn(\"features\", create_feature_vector_udf(*feature_columns))\n",
    "\n",
    "    train, test = df.randomSplit([0.9, 0.1], seed=42)\n",
    "    def evaluate_model(predictions):\n",
    "        correct_predictions = predictions.filter(predictions.label == predictions.prediction).count()\n",
    "        total_data = predictions.count()\n",
    "        accuracy = correct_predictions / total_data\n",
    "        return accuracy\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(labelCol='label', featuresCol='features')\n",
    "    lr_model = lr.fit(train)\n",
    "    lr_predictions = lr_model.transform(test)\n",
    "    lr_accuracy = evaluate_model(lr_predictions)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(labelCol='label', featuresCol='features')\n",
    "    dt_model = dt.fit(train)\n",
    "    dt_predictions = dt_model.transform(test)\n",
    "    dt_accuracy = evaluate_model(dt_predictions)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(labelCol='label', featuresCol='features')\n",
    "    rf_model = rf.fit(train)\n",
    "    rf_predictions = rf_model.transform(test)\n",
    "    rf_accuracy = evaluate_model(rf_predictions)\n",
    "\n",
    "    # Determine the best model\n",
    "    best_model_name = \"Logistic Regression\" if lr_accuracy > dt_accuracy and lr_accuracy > rf_accuracy else \\\n",
    "                    \"Decision Tree\" if dt_accuracy > rf_accuracy else \\\n",
    "                    \"Random Forest\"\n",
    "\n",
    "    best_model_accuracy = max(lr_accuracy, dt_accuracy, rf_accuracy)\n",
    "\n",
    "    print(f\"The best model is {best_model_name} with an accuracy of {best_model_accuracy:.2f}\")\n",
    "\n",
    "    # Evaluate on test data for the best model\n",
    "    if best_model_name == \"Logistic Regression\":\n",
    "        best_model = lr_model\n",
    "    elif best_model_name == \"Decision Tree\":\n",
    "        best_model = dt_model\n",
    "    else:\n",
    "        best_model = rf_model\n",
    "\n",
    "    predictions = best_model.transform(test)\n",
    "    test_accuracy = evaluate_model(predictions)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "    from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    print(\"Classification report on test set:\")\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196a6a6-2828-460e-b439-175c87acdc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
